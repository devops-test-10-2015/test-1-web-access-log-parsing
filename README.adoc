= Test 2 - Web Access Log Parsing

== Requirements

Consume web access log files and emit the following metrics:

* No. of successful requests per minute
* No. of error requests per minute
* Mean response time per minute
* MBs sent per minute

=== Data

Solution developed with this data : https://s3-eu-west-1.amazonaws.com/skyscanner-recruitement-resources/devops/access-log-example/c930ecf4b0a4426e619bddd8752c475ea772427db13eb92ee6a1a79b248ec0dc/access.log[access.log]

=== Log format

As http://httpd.apache.org/docs/2.2/mod/mod_log_config.html[Defined here] : "%a %l %u %t \"%r\" %>s %b %D"

Parse with https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns[Logstash] using `%{COMMONAPACHELOG} %{BASE10NUM:time_to_serve}`.


== How to run

=== Environment prerequisite

To produce the reports from the logs you need the following installed:

|===
|Technology | Version tested aginst

|Java
|openjdk version "1.8.0_60"

|Logstash
|1.5.4

|===


=== Create the report for stats per minute

The report is created using a couple of different tasks so these are pulled together with Gradle. You do not need to have
Gradle installed as it is being run using the wrapper.

Running the parser against the JSON version of the log data will produce a report to standard out with the metrics for each minute.

The report format is line delimited JSON.

The `jsonData` property can be supplied if you wish to override the value in `gradle.properties`.

    ./gradlew -q createAccessLogReport [-PapacheLog=test_data/20_200s_over_2_minutes.log]



    ./gradlew -q createAccessLogReport -PapacheLog=data/access.log

=== Testing

    ./gradlew test

== Decisions

=== Technology

Going with Logstash for the parsing as parsing Apache logs is a standard task
for Logstash and is tried and tested.

I would like to have done the stats gathering in Elastic Search but I could not
make that work in the available time.


== Plan

* Get the sample logs
* Write the requirements
* Decide technology for solution
* Create logstash conf to parse the logs
* Set up testing
* Get the numbers
* Done =========================================
* Add any missing documentation

=== Getting the numbers

To get the information from the data try these solutions in order:

* Logstash metrics plugin
** The default seems to be that this will give you rates based on when entries are added to the logs. I cannot find a way to use the timestamp in the log.
** I have found a way to set the Logstash timestamp to the timestamp value from the original log but the rates are per second.
** I looked through all the other filter plugins and could not find anything suitable.
* Elastic Search
** I tried this using aggregation but I could not work out how to get the values a per minute.
** I am going to give up and go with the scripting option.
* take the json output from logstash and do write a script

=== Testing

Take a small part of the log and manually count the up the values and use that
for testing.